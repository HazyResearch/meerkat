{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ef3779",
   "metadata": {},
   "source": [
    "# Tutorial 0: Data Frames for Unstructured Data\n",
    "\n",
    "In this tutorial, we'll learn how to use Meerkat to explore a dataset of images. We'll use the `imagenette` dataset, a small 10-class subset of ImageNet.\n",
    "\n",
    "Through this tutorial, you will learn about:\n",
    "- the concept of **data frames** in Meerkat\n",
    "- the different types of data that you can store in a Meerkat data frame\n",
    "- how to use **data frames** to explore unstructured data\n",
    "\n",
    "The content of this tutorial is provided in the `tutorial-0.ipynb` Jupyter notebook.\n",
    "\n",
    "## 🔮 Importing Meerkat\n",
    "Let's start by importing Meerkat.\n",
    "\n",
    "```{margin}\n",
    "The first time you import Meerkat after installation may take upto a minute, due to dependencies like `pandas`, `numpy`, and `torch`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1f722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import meerkat as mk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecdce0",
   "metadata": {},
   "source": [
    "## 💾 Downloading the data\n",
    "First, we'll download some data to explore. We're going to use the [Imagenette dataset](https://github.com/fastai/imagenette#image%E7%BD%91), a small subset of the original [ImageNet](https://www.image-net.org/update-mar-11-2021.php). \n",
    "This dataset is made up of 10 classes (e.g. \"garbage truck\", \"gas pump\", \"golf ball\").\n",
    "- Download time: <1 minute\n",
    "- Download size: 130M\n",
    "\n",
    "In addition to downloading the data, `download_imagnette` prepares a CSV, `imagenette.csv`, with a row for each image. \n",
    "\n",
    "````{margin}\n",
    "Meerkat lets you load in any dataset from [Huggingface Datasets](https://huggingface.co/docs/datasets/index).\n",
    "```python\n",
    "mk.get(\"dataset_name\", registry=\"huggingface\")\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a791e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6358ff13ddb44278989fbf6963027e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/99.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dataset_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdownload_imagenette\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m~/work/meerkat/meerkat/meerkat/datasets/imagenette/__init__.py:126\u001b[0m, in \u001b[0;36mdownload_imagenette\u001b[0;34m(download_dir, version, overwrite, return_df)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path), dir_path) \u001b[38;5;28;01mif\u001b[39;00m return_df \u001b[38;5;28;01melse\u001b[39;00m dir_path\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dir_path):\n\u001b[0;32m--> 126\u001b[0m     cached_tar_path \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimagenette\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVERSION_TO_URL\u001b[49m\u001b[43m[\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting tar archive, this may take a few minutes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m     tar \u001b[38;5;241m=\u001b[39m tarfile\u001b[38;5;241m.\u001b[39mopen(cached_tar_path)\n",
      "File \u001b[0;32m~/work/meerkat/meerkat/meerkat/datasets/utils.py:13\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, dataset_dir, force)\u001b[0m\n\u001b[1;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dataset_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_from_cache\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdownloads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/datasets/utils/file_utils.py:575\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001b[0m\n\u001b[1;32m    573\u001b[0m         ftp_get(url, temp_file)\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    587\u001b[0m shutil\u001b[38;5;241m.\u001b[39mmove(temp_file\u001b[38;5;241m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/datasets/utils/file_utils.py:379\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[1;32m    370\u001b[0m total \u001b[38;5;241m=\u001b[39m resume_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(content_length) \u001b[38;5;28;01mif\u001b[39;00m content_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m    372\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    373\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m    378\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    380\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m    381\u001b[0m         temp_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/http/client.py:459\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 459\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/http/client.py:503\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    498\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 503\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/hostedtoolcache/Python/3.8.16/x64/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from meerkat.datasets.imagenette import download_imagenette\n",
    "\n",
    "dataset_dir = \"./downloads\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "download_imagenette(dataset_dir, overwrite=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df90c6c",
   "metadata": {},
   "source": [
    "Let's take a look at the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ecaa6",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "!head -n 5 downloads/imagenette2-160/imagenette.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a6814",
   "metadata": {},
   "source": [
    "Next, we'll load it into a Meerkat `DataFrame`.\n",
    "\n",
    "## 📸 Creating an image `DataFrame`\n",
    "*For more information on creating DataFrames from various data sources, see the user guide section on {ref}`guide/dataframe/io`.*\n",
    "\n",
    "Meerkat's core contribution is the DataFrame, a simple columnar data abstraction. The Meerkat DataFrame can house columns of arbitrary type – from integers and strings to complex, high-dimensional objects like videos, images, medical volumes and graphs.\n",
    "\n",
    "We're going to build a `DataFrame` out of the `imagenette.csv` file from the download above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414aabe4",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a `DataFrame`\n",
    "df = mk.from_csv(\"./downloads/imagenette2-160/imagenette.csv\")\n",
    "\n",
    "# Create an `ImageColumn`` and add it to the `DataFrame`\n",
    "df[\"img\"] = mk.image(\n",
    "    df[\"img_path\"], \n",
    "    base_dir=os.path.join(dataset_dir, \"imagenette2-160\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59953c",
   "metadata": {},
   "source": [
    "The call to `head` shows the first few rows in the `DataFrame`. You can see that there are a few metadata columns, as well as the \"img\" column we added in.\n",
    "\n",
    "## 🗂 Selecting data\n",
    "*For more information see the user guide section on {ref}`guide/dataframe/selection`.*\n",
    "\n",
    "When we create an `ImageColumn` we don't load the images into memory. Instead, `ImageColumn` keeps track of all those filepaths we passed in and only loads the images when they are needed. \n",
    "\n",
    "When we select a row of the `ImageColumn`, we get an instance `FileCell` back. A `FileCell` is an object that holds everything we need to materialize the cell (e.g. the filepath to the image and the loading function), but stops just short of doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cell = df[\"img\"][0]\n",
    "print(f\"Indexing the `ImageColumn` returns an object of type: {type(img_cell)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc551676",
   "metadata": {},
   "source": [
    "To actually materialize the image, we simply call the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b20862",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_cell()\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ac062",
   "metadata": {},
   "source": [
    "We can subselect a **batch** of images by indexing with a slice.  Notice that this returns a smaller {class}`~meerkat.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = df[\"img\"][1:4]\n",
    "print(f\"Indexing a slice of the `ImageColumn` returns a: {type(imgs)}.\")\n",
    "imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb1986b",
   "metadata": {},
   "source": [
    "The whole batch of images can be loaded together by calling the column. \n",
    "```\n",
    "imgs();\n",
    "```\n",
    "\n",
    "One can load multiple rows using any one of following indexing schemes:\n",
    "- **Slice indexing**: _e.g._ `column[4:10]`\n",
    "- **Integer array indexing**: _e.g._ `column[[0, 4, 6, 11]]`\n",
    "- **Boolean array indexing**: _e.g._ `column[np.array([True, False, False ..., True, False])]`\n",
    "\n",
    "### 📎 _Aside_: `ImageColumn` under the hood, `DeferredColumn`.\n",
    "\n",
    "If you check out the implementation of `ImageColumn` (at [meerkat/column/image_column.py](https://github.com/HazyResearch/meerkat/blob/main/meerkat/column/image_column.py)), you'll notice that it's a super simple subclass of `DeferredColumn`. \n",
    "\n",
    "_What's a `DeferredColumn`?_\n",
    "In `meerkat`, high-dimensional data types like images and videos are typically stored in a `DeferredColumn`. A  `DeferredColumn` wraps around another column and lazily applies a function to it's content as it is indexed. Consider the following example, where we create a simple `meerkat` column..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "  col = mk.column([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c36c5",
   "metadata": {},
   "source": [
    "...and wrap it in a deferred column.\n",
    "```\n",
    "  dcol = col.defer(fn=lambda x: x + 10)\n",
    "  dcol[1]()  # the function is only called at this point!\n",
    "```\n",
    "Critically, the function inside a lambda column is only called at the time the column is called! This is very useful for columns with large data types that we don't want to load all into memory at once. For example, we could create a `DeferredColumn` that lazily loads images...\n",
    "```\n",
    "  >>> filepath_col = mk.PandasSeriesColumn([\"path/to/image0.jpg\", ...])\n",
    "  >>> img_col = filepath.defer(lambda x: load_image(x))\n",
    "```\n",
    "An `ImageColumn` is a just a `DeferredColumn` like this one, with a few more bells and whistles!\n",
    "\n",
    "## 🛠 Applying operations over the DataFrame.\n",
    "\n",
    "When analyzing data, we often perform operations on each example in our dataset (e.g. compute a model's prediction on each example, tokenize each sentence, compute a model's embedding for each example) and store them. The `DataFrame` makes it easy to perform these operations:  \n",
    "- Produce new columns (via `DataFrame.map`)\n",
    "- Produce new columns and store the columns alongside the original data (via `DataFrame.update`)\n",
    "- Extract an important subset of the datset (via `DataFrame.filter`).   \n",
    "\n",
    "Under the hood, dataloading is multiprocessed so that costly I/O doesn't bottleneck our computation.\n",
    "\n",
    "Let's start by filtering the `DataFrame` down to the examples in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c05e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = df.filter(lambda split: split == \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f8618",
   "metadata": {},
   "source": [
    "### 🫐  Using `DataFrame.map` to compute average intensity of the blue color channel in the images.\n",
    "\n",
    "To demonstrate the utility of the `map` operation, we'll explore the relationship between the \"blueness\" of an image and the class of the image. \n",
    "\n",
    "We'll quantify the \"blueness\" of each image by simply computing the mean intensity of the blue color channel. This can be accomplished with a simple `map` operation over the `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_col = valid_df.map(\n",
    "    lambda img: np.array(img)[:, :, 2].mean(), \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Add the intensities as a new column in the `DataFrame` \n",
    "valid_df[\"avg_blue\"] = blue_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04f91c",
   "metadata": {},
   "source": [
    "### 🪂 vs. ⛳️\n",
    "Next, we'll explore the relationship between blueness and the class label of the image. To do so, we'll compare the blue intensity distribution of images labeled \"parachute\" to the distribution of of images labeled \"golf ball\".\n",
    "Using the [`seaborn`](https://seaborn.pydata.org/installing.html) plotting package and our `DataFrame`, this can be accomplished in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704e3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: this cell requires the seaborn dependency: https://seaborn.pydata.org/installing.html \n",
    "import seaborn as sns\n",
    "\n",
    "plot_df = valid_df[np.isin(valid_df[\"label\"], [\"golf ball\", \"parachute\"])]\n",
    "sns.displot(\n",
    "    data=plot_df.to_pandas(), \n",
    "    x=\"avg_blue\", \n",
    "    hue=\"label\", \n",
    "    height=3, \n",
    "    aspect=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[\"img\"][int(np.argmax(valid_df[\"avg_blue\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82c60a",
   "metadata": {},
   "source": [
    "## 📉 ML with images in `meerkat`.\n",
    "\n",
    "Let's do some machine learning on our Imagenette `DataFrame`.\n",
    "We'll take a resnet18 pretrained on the full ImageNet dataset, perform inference on the validation set, and analyze the model's predictions and activations. \n",
    "\n",
    "The cell below downloads the model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64025af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torchvision.transforms as transforms\n",
    "model = resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb1937",
   "metadata": {},
   "source": [
    "### 💈  Applying a transform to the images.\n",
    "In order to do inference, we'll need to create a _new_ {class}`~meerkat.DeferredColumn`. The `ImageColumn` we defined above (_i.e._ `\"img_path\"`), does not apply any transforms after loading and simply returns a PIL image. Before passing the images through the model, we need to convert the PIL image to a `torch.Tensor` and normalize the color channels (along with a few other transformations). \n",
    "\n",
    "Note: the transforms defined below are the same as the ones used by torchvision, see [here](https://github.com/pytorch/examples/blob/cbb760d5e50a03df667cdc32a61f75ac28e11cbf/imagenet/main.py#L225). \n",
    "\n",
    "In the cell below, we specify a transform when creating the `ImageColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbacd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create new column with transform \n",
    "valid_df[\"input\"] = valid_df[\"img\"].defer(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc4287",
   "metadata": {},
   "source": [
    "Notice that indexing this new column returns a `torch.Tensor`, not a PIL image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = valid_df[\"input\"][0]()\n",
    "print(f\"Indexing the `ImageColumn` returns an object of type: {type(img)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cb64d",
   "metadata": {},
   "source": [
    "... and that indexing a slice of this new column returns a {class}`~meerkat.TensorColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1673bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = img = valid_df[\"input\"][:3]()\n",
    "print(f\"Indexing a slice of the `ImageColumn` returns an object of type: {type(img)}.\")\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54adb4",
   "metadata": {},
   "source": [
    "Let's see what the full `DataFrame` looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ca5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c1ec2",
   "metadata": {},
   "source": [
    "### 💫 Computing model predictions and activations.\n",
    "We'd like to perform inference and extract:\n",
    "  \n",
    "1. Output predictions  \n",
    "2. Output class probabilities  \n",
    "3. Model activations \n",
    "\n",
    "Note: in order to extract model activations, we'll need to use a [PyTorch forward hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) and register it on the final layer of the ResNet. Forward hooks are just functions that get executed on the forward pass of a `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91445d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define forward hook in ActivationExtractor class\n",
    "class ActivationExtractor:\n",
    "    \"\"\"Extracting activations a targetted intermediate layer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = None\n",
    "\n",
    "    def forward_hook(self, module, input, output):\n",
    "        self.activation = output\n",
    "\n",
    "# Register forward hook\n",
    "extractor = ActivationExtractor()\n",
    "model.layer4.register_forward_hook(extractor.forward_hook);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cf380",
   "metadata": {},
   "source": [
    "We want to apply a forward pass to each image in the `DataFrame` and store the outputs as new columns: `DataFrame.map` is perfectly suited for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bfc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Move the model to GPU, if available\n",
    "# device = 0\n",
    "device = \"cpu\"\n",
    "model.to(device).eval()\n",
    "\n",
    "# 2. Define a function that runs a forward pass over a batch \n",
    "@torch.no_grad()\n",
    "def predict(input: mk.TensorColumn):\n",
    "    x: torch.Tensor = input.data.to(device)  # We get the underlying torch tensor with `data` and move to GPU \n",
    "    out: torch.Tensor = model(x)  # Run forward pass\n",
    "\n",
    "    # Return a dictionary with one key for each of the new columns. Each value in the\n",
    "    # dictionary should have the same length as the batch. \n",
    "    return {\n",
    "        \"pred\": out.cpu().numpy().argmax(axis=-1),\n",
    "        \"probs\": torch.softmax(out, axis=-1).cpu(),\n",
    "        \"activation\": extractor.activation.mean(dim=[-1,-2]).cpu()\n",
    "    }\n",
    "# 3. Apply the update. Note that the `predict` function operates on batches, so we set \n",
    "# `batched=True`. Also, the `predict` function only accesses the \"input\" column, by \n",
    "# specifying that here we instruct update to only load that one column and skip others \n",
    "pred_df = valid_df.map(function=predict, is_batched_fn=True, batch_size=32)\n",
    "valid_df = mk.concat([valid_df, pred_df], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afcf1a0",
   "metadata": {},
   "source": [
    "The predictions, output probabilities, and activations are now stored alongside the examples in the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[[\"label_id\", \"input\", \"pred\", \"probs\", \"activation\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5236177",
   "metadata": {},
   "source": [
    "### 🎯  Computing metrics and analyzing performance. \n",
    "\n",
    "Computing statistics on Meerkat `DataFrames` is straightforward because standard NumPy operators and functions can be applied directly to a `NumpyArrayColumn`. We take advantage of this below to compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbdfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[\"correct\"] = valid_df[\"pred\"] == valid_df[\"label_idx\"].data\n",
    "accuracy = valid_df[\"correct\"].mean()\n",
    "print(f\"Micro accuracy across the ten Imagenette classes: {accuracy:0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a80fa2",
   "metadata": {},
   "source": [
    "Furthermore, since the `DataFrame` is naturally converted to a Pandas DataFrame, it's easy to use data visualization tools that interface with Pandas (_e.g._ seaborn, bokeh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd747983",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: this cell requires the seaborn dependency: https://seaborn.pydata.org/installing.html \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.barplot(data=valid_df.to_pandas(), y=\"label\", x=\"correct\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab326b",
   "metadata": {},
   "source": [
    "### 🔎  Exploring model activations.\n",
    "To better understand the behavior of our model, we'll explore the activations of the final convolutional layer of the ResNet. Recall that when we performed our forward pass, we extracted these activations and stored them in a new column called `\"activation\"`.\n",
    "\n",
    "Unlike the the `NumpyArrayColumn`s we've been working with so far, the activation column has an additional dimension of size 512.\n",
    "\n",
    "To visualize the activations, we'll use a dimensionality reduction technique ([UMAP](https://umap-learn.readthedocs.io/en/latest/)) to embed the activations in two dimensions. We'll store these embeddings in two new columns \"umap_0\" and \"umap_1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ab75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPTIONAL: this cell requires the umap dependency: https://umap-learn.readthedocs.io/en/latest/\n",
    "!pip install umap-learn\n",
    "from umap import UMAP\n",
    "\n",
    "# 1. Compute UMAP embedding\n",
    "reducer = UMAP()\n",
    "embs = reducer.fit_transform(valid_df[\"activation\"])\n",
    "\n",
    "# 2. Add the embedding to the DataFrame as two new columns \n",
    "valid_df[\"umap_0\"] = embs[:, 0]\n",
    "valid_df[\"umap_1\"] = embs[:, 1]\n",
    "\n",
    "## OPTIONAL: this cell requires the seaborn dependency: https://seaborn.pydata.org/installing.html \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(data=valid_df.to_pandas(), x=\"umap_0\", y=\"umap_1\", hue=\"label\");\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aca439",
   "metadata": {},
   "source": [
    "## 💾  Writing a `DataFrame` to disk. \n",
    "Finally, we can write the updated `DataFrame`, with all the activations and predictions included, to disk for later use.  \n",
    "\n",
    "`DataFrame` size on disk: 25MB\n",
    "\n",
    "On disk, the `DataFrame` is stored in a directory at the path passed to `DataFrame.write`. Within that directory, each column is stored separately. This allows us to read only a subset of columns from DataFrame on disk. Use the file explorer to the left to further expore the file structure of the `DataFrame`.  \n",
    "```\n",
    "valid_df\n",
    "|  +-- meta.yml   \n",
    "|  +-- state.dill  \n",
    "|  +-- columns\n",
    "   |   +-- activation\n",
    "   |   +-- avg_blue\n",
    "   |   +-- correct\n",
    "   |   ...\n",
    "   |   +-- umap_1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.write(os.path.join(dataset_dir, \"valid_df\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = mk.read(os.path.join(dataset_dir, \"valid_df\"))"
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "source_map": [
   5,
   25,
   29,
   46,
   52,
   56,
   60,
   70,
   82,
   91,
   94,
   97,
   100,
   103,
   107,
   125,
   127,
   150,
   152,
   160,
   168,
   174,
   188,
   190,
   199,
   204,
   213,
   226,
   229,
   232,
   235,
   239,
   242,
   244,
   255,
   269,
   273,
   297,
   301,
   303,
   309,
   313,
   317,
   322,
   331,
   349,
   369,
   373
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}