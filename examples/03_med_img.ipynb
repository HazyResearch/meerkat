{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "03-med_img.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "c69732a7865039e91888aaead3ff2569340c124c546c64a2f5d6518a2cd9a0e4"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('domino': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmHR9PFqhm5D"
      },
      "source": [
        "# Working with medical images in Meerkat\n",
        "\n",
        "To motivate Meerkat, let's consider the task of detecting pneumothorax (*i.e.* a collapsed lung) in chest X-rays ([Irvin *et al.*](https://arxiv.org/pdf/1901.07031.pdf), [Taylor *et al.*](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002697)). As we develop a model for this task, we encounter data of different types – from X-ray images to structured metadata to embeddings extracted from a trained model. Meerkat provides the `DataPanel`, a columnar data structure (similar to a Pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)) that can house all of these data under one roof. Keeping them together enables quicker, more adventurous model iteration, fine-grained error analysis, and easier data exploration and inspection.\n",
        "\n",
        "**Time**: ~20 minutes\n",
        "\n",
        "**Colab Runtime**: We recommend running this Colab with a GPU runtime. To change the runtime, \n",
        "1. Click on `Runtime` on the top navigation bar\n",
        "2. Select `Change runtime type`\n",
        "3. Select `GPU` from the dropdown\n",
        "\n",
        "**TODOs**\n",
        "- Remove kaggle username/token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78JQBrphqaV"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UvqVJfZtdL4I",
        "outputId": "5ed2cc9b-4b1d-4b2c-aa16-4ff8326f99dc"
      },
      "source": [
        "!pip install -q meerkat-ml[medimg,text]\n",
        "!pip install kaggle\n",
        "!pip install -q torchxrayvision\n",
        "!pip install umap-learn\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "\n",
        "import meerkat.version as mversion\n",
        "import torch\n",
        "print(\"meerkat version: \", mversion.__version__)\n",
        "print(\"torch version: \", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 163kB 7.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 839kB 48.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 35.5MB 88kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 34.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 38.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 235kB 60.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.1MB 41.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 59.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 430kB 46.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[?25h  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for Pmw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nested-lookup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.61.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.22.2.post1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.61.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-97a4b4d4939e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python3 -m spacy download en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmeerkat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meerkat version: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/meerkat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeerkat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvolume_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMedicalVolumeColumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeerkat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapanel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataPanel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmeerkat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeerkat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeerkat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovenance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprovenance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'meerkat.ops'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4zZE8XViu79"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import meerkat as mk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchxrayvision as xrv\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Uncomment the line below to see whats going on under the hood\n",
        "# logging.getLogger(\"meerkat\").setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kafz9iimgPe2"
      },
      "source": [
        "## 💾 Downloading the data\n",
        "We'll be using the dataset from the [SIIM-ACR Pneumothorax Segmentation Challenge](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/data) (`mosaic.contrib.siim_cxr` provides utility functions for downloading the data). The downloaded dataset includes the inputs, a large number of chest x-ray files stored in [DICOM](https://www.dicomstandard.org/) format, and the targets, a CSV file mapping each file to its binary pneumothorax label.\n",
        "-  Download time: ~2 minutes\n",
        "- Download size:  2.0 GB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgIfg0IFNvnb"
      },
      "source": [
        "from meerkat.contrib.siim_cxr import download_siim_cxr\n",
        "download_siim_cxr(\n",
        "    \"./\", \n",
        "    kaggle_username=\"sabrieyuboglu\", \n",
        "    kaggle_key=\"8124277674a280e445d0c7c0ed769fd3\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbinUeSQ596i"
      },
      "source": [
        "## 🔨 Building a `DataPanel`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3vye4H-iOf0"
      },
      "source": [
        "dp = mk.DataPanel.from_csv(\"siim_cxr.csv\")\n",
        "dp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1gH9JLhjpVM"
      },
      "source": [
        "So far, the DataPanel isn't providing anything we couldn't get with a Pandas `DataFrame` because the columns in the CSV include only strings and numbers. \n",
        "\n",
        "Things get interesting when we start adding columns for objects that don't play nicely with Pandas – things like images, text, time-series, videos, and multi-dimensional arrays. Out-of-the-box, Meerkat comes with a number of common column types including `ImageColumn` for images, `VideoColumn` for videos, `NumpyArrayColumn` for (potentially multi-dimensional) NumPy `ndarray`s, and `TensorColumn` for PyTorch Tensors (see [here](https://github.com/robustness-gym/meerkat/blob/dev/README.md#supported-columns) for a full list of core columns).  \n",
        "\n",
        "To house the X-rays in the dataset, we'll be using the `MedicalVolumeColumn`, a column type similar to `ImageColumn` but optimized for medical images stored in [DICOM format](https://www.dicomstandard.org/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQkKc7mrjT9A"
      },
      "source": [
        "# Make a column of MedicalVolumeCells\n",
        "from dosma import DicomReader\n",
        "from meerkat.contrib.siim_cxr import cxr_transform, cxr_transform_pil\n",
        "\n",
        "loader = DicomReader(group_by=None, default_ornt=(\"SI\", \"AP\"))\n",
        "dp[\"img\"] = mk.MedicalVolumeColumn.from_filepaths(\n",
        "    dp[\"filepath\"], loader=loader, transform=cxr_transform_pil\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKW6mJLD596k"
      },
      "source": [
        "## 📄 Adding in metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVXoMVL5JWP2"
      },
      "source": [
        "def unroll_metadata(dp):\n",
        "    return dp[\"img\"].get_metadata(\n",
        "        as_raw_type=True,\n",
        "        readable=True,\n",
        "        ignore_bytes=True,\n",
        "        force_load=True,\n",
        "    )\n",
        "\n",
        "dp = dp.update(unroll_metadata, materialize=False, pbar=True)\n",
        "dp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko-FWCct596l"
      },
      "source": [
        "### 💫 Computing model predictions and activations.\n",
        "We'd like to perform inference and extract:\n",
        "  \n",
        "1. Output predictions  \n",
        "2. Output class probabilities  \n",
        "3. Model activations \n",
        "\n",
        "Note: in order to extract model activations, we'll need to use a [PyTorch forward hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) and register it on the final layer of the ResNet. Forward hooks are just functions that get executed on the forward pass of a `torch.nn.Module`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bNeVWFRCCqE"
      },
      "source": [
        "model = xrv.models.DenseNet(weights=\"chex\").to(\"cuda\")\n",
        "\n",
        "class_to_idx = {\n",
        "    label: idx for idx, label in \n",
        "    enumerate(xrv.models.model_urls[\"chex\"][\"labels\"])\n",
        "}\n",
        "model.eval()\n",
        "\n",
        "# 2. Register the forward hook\n",
        "embedding = None\n",
        "def forward_hook(module, input, output):\n",
        "  global embedding\n",
        "  embedding = output\n",
        "\n",
        "model.features.register_forward_hook(forward_hook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCIPq7TiC1k4"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "  transforms.Lambda(lambda x: np.array(cxr_transform_pil(x))),\n",
        "  transforms.Lambda(lambda x: xrv.datasets.normalize(x, 255)[None, :, :]),\n",
        "  xrv.datasets.XRayCenterCrop(),\n",
        "  xrv.datasets.XRayResizer(224), \n",
        "  transforms.Lambda(lambda x: torch.tensor(x)),\n",
        "])\n",
        "\n",
        "dp[\"input\"] = mk.MedicalVolumeColumn.from_filepaths(\n",
        "    dp[\"filepath\"], loader=loader, transform=transform\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1EkMJcS596m"
      },
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(batch: mk.DataPanel):\n",
        "  global embedding\n",
        "  x = batch[\"input\"].data.to(\"cuda\") \n",
        "  out = model(x)  # Run forward pass\n",
        "\n",
        "  return {\n",
        "       \"output\": mk.ClassificationOutputColumn(probs=out.cpu(), multi_label=True),\n",
        "       \"embedding\": mk.EmbeddingColumn(embedding.mean(dim=[-1,-2]).cpu())\n",
        "  }\n",
        "\n",
        "dp = dp.update(\n",
        "  function=predict, is_batched_fn=True, batch_size=16,\n",
        "  num_workers=2, pbar=True, input_columns=[\"input\"] \n",
        ")\n",
        "dp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcEQ_Fk14l8C"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(dp[\"pmx\"].data, dp[\"output\"].probabilities().data[:, class_to_idx[\"Pneumothorax\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgJs1REI4e4M"
      },
      "source": [
        "umap = dp[\"embedding\"].umap()\n",
        "\n",
        "dp[\"umap_0\"] = umap.embeddings[:, 0]\n",
        "dp[\"umap_1\"] = umap.embeddings[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhUuR6995DBl"
      },
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(4,4))\n",
        "sns.scatterplot(\n",
        "    data=dp.lz[:1000].to_pandas(), \n",
        "    x=\"umap_0\", \n",
        "    y=\"umap_1\", \n",
        "    hue=\"Patient's Sex\",\n",
        "    #alpha=0.05\n",
        ")\n",
        "sns.despine()\n",
        "# plt.savefig(\"fig.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p13DXjxO596n"
      },
      "source": [
        "### 📄 Radiologist reports (`SpacyColumn`)\n",
        "\n",
        "In pneuomothorax detection, as in other classification tasks, the binary label does not capture all of the nuance in the X-ray. Radiologists communicate that additional detail via natural language radiologist reports that accompany each scan. For example, a sentence in a chest X-ray report may read \"A medial pneumothorax is present adjacent to the heart.\" Increasingly, these reports are playing a starring role in machine learning for medical imaging. The reports are used to extract weak labels ([Dunnmon & Ratner, et al.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7413132/), [Eyuboglu *et al.*](https://www.nature.com/articles/s41467-021-22018-1)) and perform contrastive learning on paired images and reports ([Zhang *et al.*](https://arxiv.org/pdf/2010.00747.pdf)). With Meerkat, we can store the radiology reports right alongside the X-rays in the same `DataPanel`. This allows us to experiment with multi-modal learning techniques without re-engineering our data pipelines. \n",
        "\n",
        "Additionally, we can use the accompanying radiology reports to select critical subsets of the data and compute subgroup accuracy. For instance, say we're interested in the performance of our model on \"severe\" pneumothorax. Because the radiologist reports are stored in a `SpacyColumn`, a column that holds preprocessed (*e.g.* tokenized) natural language data, it's easy to write a function `is_severe` that accepts a row as input and returns `True` if the X-ray exhibits pneumothorax and the words \"pneumothorax\" and \"severe\" appear in the same sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWK2NvIb596n"
      },
      "source": [
        " dp[\"report_doc\"] = mk.SpacyColumn.from_texts(dp[\"report\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8FcXi71596n"
      },
      "source": [
        "def is_severe(row: mk.DataPanel):\n",
        "\t\"\"\" Return `True` if the X-ray exhibits pneumothorax and it is described \n",
        "\tas severe in the report (according to a simple rule-based heuristic).\"\"\"\n",
        "\tif row[\"pmx\"] != 1:\n",
        "\t\treturn False\n",
        "\tfor sent in row[\"report_doc\"].sents:\n",
        "\t\tif \"pneumothorax\" in str(sent) and \"severe\" in str(sent):\n",
        "\t\t\treturn True\n",
        "\treturn False\n",
        "\n",
        "severe_dp = dp.filter(\n",
        "\tfunction=is_severe, is_batched_fn=False, input_columns=[\"report_doc\", \"pmx\"], pbar=True\n",
        ")\n",
        "\n",
        "print(f\"There are {len(severe_dp)} X-rays exhibiting severe pneumothorax.\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2MzA1P1596o"
      },
      "source": [
        "## 👓  Radiologist eye-tracking data (`GazeSequenceCell` and `CellColumn`)  \n",
        "\n",
        "Our chest X-ray dataset includes an exciting, non-standard data modality, eye-tracking time-series, for which we'll implement a custom column. We have access to this data because a subset of the X-rays in the dataset were labeled by radiologists while their gaze was being recorded by an eye-tracker. This gaze signal can provide additional supervision when training a model or can be used to slice the dataset during evaluation.\n",
        "\n",
        "Meerkat does **not** ship with a column type for eye-tracking data, so we'll have to write our own. In Meerkat, the easiest way to implement a new column is to use the `CellColumn` abstraction. The advantage of using `CellColumn` (or one of its subclasses) is that we can support new data types without dealing with the implementation complexity of a full column. Instead, we can think in terms of the individual elements in the column: the cells. We implement a cell by subclassing `AbstractCell` and adding functionality specific to the new data type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4Ox0AvN596p"
      },
      "source": [
        "# The gaze data stored in JSON format\n",
        "import json\n",
        "gaze_data = json.load(open(\"cxr_gaze_data.json\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XStCWzBy596p"
      },
      "source": [
        "Below, we provide a simple implementation of a new cell type GazeSequenceCell that houses a sequence of eye-tracking coordinates. In addition to adding `__repr__` and `_state_keys` methods, useful for column inspection and serialization respectively, we implement the utility method to_gaze_heatmap which produces a NumPy array representing the amount of time the radiologist's gaze fell on each patch of the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJl5jT4jPsGW"
      },
      "source": [
        "from typing import Sequence\n",
        "\n",
        "class GazeSequenceCell(mk.AbstractCell):\n",
        "\n",
        "  def __init__(self, gaze_x: Sequence, gaze_y: Sequence, time: Sequence):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        gaze_x (Sequence): \n",
        "        gaze_y (Sequence): [description]\n",
        "        time (Sequence): [description]\n",
        "    \"\"\"\n",
        "    self.gaze_coordinates = np.array([gaze_y, gaze_x])\n",
        "    self.time = np.array(time)\n",
        "  \n",
        "  def get(self):\n",
        "    return self\n",
        "  \n",
        "  def to_heatmap(self, num_patches: int = 16) -> np.ndarray:\n",
        "    \"\"\" Convert the sequence to a heatmap showing the cumulative\n",
        "    duration that the gaze fell on each patch of the image.  \n",
        "    Args:\n",
        "      num_patches (int): split the image into `num_patches` x `num_patches`\n",
        "        patches.\n",
        "    Returns:\n",
        "      np.ndarray: an array with shape (num_patches, num_patches) where   \n",
        "    \"\"\"\n",
        "    heatmap = np.zeros(num_patches * num_patches)\n",
        "    patches = (\n",
        "        np.floor(self.gaze_coordinates[0] * num_patches) * num_patches + \n",
        "        np.floor(self.gaze_coordinates[1] * num_patches)\n",
        "    )\n",
        "    np.add.at(heatmap, patches.astype(int), self.time)\n",
        "    return heatmap.reshape(num_patches, num_patches)\n",
        "  \n",
        "  def __repr__(self):\n",
        "      return f\"GazeSequence(length={self.gaze_coordinates.shape[-1]})\"\n",
        "\n",
        "  @classmethod\n",
        "  def _state_keys(cls):\n",
        "      return {\"gaze_coordinates\", \"time\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3PMEvgh596q"
      },
      "source": [
        "We create a full column by instantiating a GazeSequenceCell for each X-ray and passing them into a new CellColumn.  Because we only have gaze data for a subset of the X-rays in the dataset, we store the gaze sequences in a new DataPanel alongside their corresponding \"image_id\" and then perform a database style join (via ms.merge) to combine the original DataPanel with the gaze data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWGPesxU-ftj"
      },
      "source": [
        "image_ids, cells = zip(*[\n",
        "    (row[\"image_id\"], GazeSequenceCell(row[\"gaze_x\"], row[\"gaze_y\"], row[\"time\"])) \n",
        "    for row in gaze_data\n",
        "])\n",
        "gaze_dp = mk.DataPanel.from_batch({\n",
        "    \"gaze\": mk.CellColumn.from_cells(cells),\n",
        "    \"image_id\": mk.NumpyArrayColumn(image_ids)\n",
        "})\n",
        "gaze_dp = mk.merge(dp, gaze_dp, how=\"inner\", on=\"image_id\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HKzM-oKCAt7"
      },
      "source": [
        "NUM_PATCHES = 16\n",
        "row = gaze_dp[4]\n",
        "heatmap = row[\"gaze\"].to_heatmap(num_patches=NUM_PATCHES)\n",
        "height, width = np.array(row[\"img\"]).shape\n",
        "plt.imshow(row[\"img\"], cmap=\"gray\")\n",
        "plt.imshow(\n",
        "    heatmap.repeat(height / NUM_PATCHES, axis=0).repeat(width / NUM_PATCHES, axis=1), \n",
        "    alpha=0.4\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbe_7iRORkv8"
      },
      "source": [
        "dp[[\"image_id\", \"pmx\", \"filepath\", \"Patient's Age\", \"Patient's Sex\", \"img\", \"output\", \"embedding\", \"umap_0\", \"umap_1\", \"report_doc\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU37Fy-O6n2y"
      },
      "source": [
        "gaze_dp[\"patient_age\"] = np.array(gaze_dp[\"Patient's Age\"])\n",
        "gaze_dp[\"patient_sex\"] = np.array(gaze_dp[\"Patient's Sex\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w8abaJJ69RK"
      },
      "source": [
        "gaze_dp[[\"image_id\", \"pmx\", \"filepath\", \"patient_age\", \"patient_sex\", \"img\", \"output\", \"embedding\", \"umap_0\", \"umap_1\", \"report_doc\", \"gaze\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg6EZ3WB596r"
      },
      "source": [
        "## ✂️ Segmentations\n",
        "Segmentations are useful for systematically communicating regions of interest (ROIs) in an image. These annotations can also help with standardized reporting and comparison of quantitative values. However, segmentations can be quite expensive to collect and difficult to interact with dynamically.\n",
        "\n",
        "Meerkat simplies the storage and dynamic interation with these visual labels. For example, we can use these segmentations to compute quantitative metrics, such as ROI. We can also visually compare the segmentations with Gaze heatmaps to qualitatively inspect how well gaze data can be used as a corollary for segmentations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJdikltLpmR6"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def rle2mask(rle, orig_dim, resize_dim = None, to_nan: bool = False):\n",
        "  \"\"\"Convert run length encoding (RLE) to 2D binary mask.\n",
        "\n",
        "  Args:\n",
        "    rle (Sequence[int]): Run length encoding.\n",
        "    orig_dim (Tuple[int]): Shape of the image.\n",
        "    resize_dim (Tuple[int]): Shape to resize to.\n",
        "      Resizing is done with cubic interporlation.\n",
        "    to_nan (bool, optional): Convert 0s to np.nan.\n",
        "\n",
        "  Returns:\n",
        "    np.ndarray: The binary mask.\n",
        "  \"\"\"\n",
        "  height, width = orig_dim\n",
        "  mask = np.zeros(width * height)\n",
        "  array = np.asarray([int(x) for x in rle.split()])\n",
        "  starts = array[0::2]\n",
        "  lengths = array[1::2]\n",
        "  current_position = 0\n",
        "\n",
        "  for index, start in enumerate(starts):\n",
        "    current_position += start\n",
        "    mask[current_position : current_position + lengths[index]] = 1\n",
        "    current_position += lengths[index]\n",
        "  mask = mask.reshape(width, height)\n",
        "\n",
        "  if resize_dim is not None:\n",
        "    mask = cv2.resize(mask, resize_dim, interpolation=cv2.INTER_CUBIC)\n",
        "  if to_nan:\n",
        "    mask[mask == 0] = np.nan\n",
        "  return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vuRW4lRER_F"
      },
      "source": [
        "def estimate_pmx_area(row):\n",
        "  \"\"\"Estimate the pneumothorax area in mm^2.\"\"\"\n",
        "  img = row[\"img\"]\n",
        "  encoded_pixels = row[\"encoded_pixels\"]\n",
        "  if encoded_pixels == \"-1\":\n",
        "    # No pneumothorax labeled\n",
        "    return {\"Area\": 0.}\n",
        "  spacing = row[\"Pixel Spacing\"]\n",
        "  pixel_area = np.prod([float(x) for x in spacing])  # Area per pixel in mm^2\n",
        "  total_area = pixel_area * np.sum(rle2mask(encoded_pixels, img.size))\n",
        "  return {\"Area\": total_area}\n",
        "\n",
        "# Compute pneumothorax ROI area for examples with pneumothorax\n",
        "dp = dp.update(\n",
        "  function=estimate_pmx_area, is_batched_fn=False, batch_size=16,\n",
        "  num_workers=2, pbar=True,\n",
        "  input_columns=[\"img\", \"encoded_pixels\", \"Pixel Spacing\"], \n",
        ")\n",
        "dp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdAOZZDFD1TK"
      },
      "source": [
        "row = dp[1]\n",
        "alpha = 0.4\n",
        "\n",
        "# Plot segmentation\n",
        "_, ax = plt.subplots(1,1, figsize=(5,5))\n",
        "ax.imshow(row[\"img\"], cmap=\"gray\")\n",
        "mask = rle2mask(row[\"encoded_pixels\"], row[\"img\"].size, to_nan=True)\n",
        "ax.imshow(mask, alpha=alpha, cmap=\"jet\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rniq9cYuZ7Lw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}